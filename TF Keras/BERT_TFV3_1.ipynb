{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_TFV3-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og-Mt0oFv6FP",
        "colab_type": "text"
      },
      "source": [
        "BERT TF script, Refactored:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fFiXAcmdoKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Progress Counter\n",
        "import sys\n",
        "\n",
        "class progressCounter():\n",
        "\n",
        "  def __init__(self, num_iterations, name=''):\n",
        "    self.progress = 0\n",
        "    self.N = num_iterations\n",
        "    self.name = name\n",
        "\n",
        "\n",
        "  def check_pt(self, i):\n",
        "    curr_progress = int(i/(self.N-1) * 100)\n",
        "    if curr_progress - self.progress > 0:\n",
        "      self.progress = curr_progress\n",
        "      sys.stdout.write(\"\\r{1} Progress: {0}%\".format(self.progress, self.name))\n",
        "      sys.stdout.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsSLSBGKeYgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate BERT from Hub\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "max_seq_length = 512  # Your choice here.\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                       name=\"input_word_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                   name=\"input_mask\")\n",
        "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                    name=\"segment_ids\")\n",
        "\n",
        "bert_inputs = [input_word_ids, input_mask, segment_ids]\n",
        "\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2\",\n",
        "                            trainable=True, name='bert_layer')\n",
        "pooled_output, sequence_output = bert_layer(bert_inputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1ITN0A3oPuf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b257fa2e-4d23-4857-a5ee-cc716913c419"
      },
      "source": [
        "# Import and Build BERT Tokenizer\n",
        "\n",
        "!pip install tf-models-official\n",
        "import official.nlp.bert.tokenization as tokenization\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "import numpy as np\n",
        "# Convert To BERT Tokens\n",
        "def convertToBERTInputData(sentence):\n",
        "  sent_tokenized = tokenizer.tokenize(sentence)\n",
        "  sent_length = len(sent_tokenized)\n",
        "  if sent_length > 510:\n",
        "    sent_tokenized = sent_tokenized[0:510]\n",
        "    sent_length = 510\n",
        "\n",
        "  sent_tokenized.insert(0,'[CLS]')\n",
        "  sent_tokenized.append('[SEP]')\n",
        "  sent_length+=2\n",
        "\n",
        "  sent_ids = tokenizer.convert_tokens_to_ids(sent_tokenized) + [0] * (512 - sent_length)\n",
        "  sent_mask = [1]*sent_length + [0]*(512 - sent_length)\n",
        "  sent_seg_ids =  [0] * 512\n",
        "\n",
        "  sent_ids = np.array(sent_ids, dtype=np.int32)\n",
        "  sent_mask = np.array(sent_mask, dtype=np.int32)\n",
        "  sent_seg_ids = np.array(sent_seg_ids, dtype=np.int32)\n",
        "\n",
        "  return ( sent_ids, sent_mask, sent_seg_ids )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-models-official\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/33/91e5e90e3e96292717245d3fe87eb3b35b07c8a2113f2da7f482040facdb/tf_models_official-2.3.0-py2.py3-none-any.whl (840kB)\n",
            "\r\u001b[K     |▍                               | 10kB 24.0MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 6.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 7.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |██                              | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 61kB 7.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |███▏                            | 81kB 8.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 92kB 8.2MB/s eta 0:00:01\r\u001b[K     |████                            | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 153kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 163kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 174kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 184kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 194kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 204kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 215kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 225kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 235kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 245kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 256kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 266kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 276kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 286kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 296kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 307kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 317kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 327kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 337kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 348kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 358kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 368kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 378kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 389kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 399kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 409kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 419kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 430kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 440kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 450kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 460kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 471kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 481kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 491kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 501kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 512kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 522kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 532kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 542kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 552kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 563kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 573kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 583kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 593kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 604kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 614kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 624kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 634kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 645kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 655kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 665kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 675kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 686kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 696kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 706kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 716kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 727kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 737kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 747kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 757kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 768kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 778kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 788kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 798kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 808kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 819kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 829kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 839kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 849kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.18.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (7.0.0)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.21.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.4.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.29.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (3.2.2)\n",
            "Requirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (2.3.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 30.0MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/2a/496e06fd289c01dc21b11970be1261c87ce1cc22d5340c14b516160822a7/opencv_python_headless-4.4.0.42-cp36-cp36m-manylinux2014_x86_64.whl (36.6MB)\n",
            "\u001b[K     |████████████████████████████████| 36.6MB 87kB/s \n",
            "\u001b[?25hRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.5.6)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.7.12)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.7)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (5.4.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (3.13)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.3.0)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.0.5)\n",
            "Collecting tensorflow-model-optimization>=0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/cc/4b0831f492396f03a4563cc749ad94cbf7af986aaa7a7d89e5979029ce5c/tensorflow_model_optimization-0.4.1-py2.py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 50.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (2.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.8.3)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.8.0)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/f5/8e6e85ce2e9f6e05040cf0d4e26f43a4718bcc4bce988b433276d4b1a5c1/py-cpuinfo-7.0.0.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.1MB/s \n",
            "\u001b[?25hCollecting tf-slim>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 50.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (1.0.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (3.12.4)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (0.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-official) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-official) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-official) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-official) (1.2.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (0.3.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (1.31.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (2.3.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.23.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official) (4.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official) (4.41.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official) (2020.6.20)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.0.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.17.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->tf-models-official) (2018.9)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.2.1->tf-models-official) (0.1.5)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (0.22.2)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (19.3.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (0.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (0.16.0)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tf-models-official) (2.7.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->tf-models-official) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->google-cloud-bigquery>=0.31.0->tf-models-official) (49.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (1.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle>=1.3.9->tf-models-official) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official) (0.2.8)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-official) (1.52.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (1.7.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official) (3.1.0)\n",
            "Building wheels for collected packages: py-cpuinfo\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-cp36-none-any.whl size=20069 sha256=040e0f8d367d67fdf49252c1fd8e7049ba168c73e42eecd1f3eac5ee77eae2e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/93/7b/127daf0c3a5a49feb2fecd468d508067c733fba5192f726ad1\n",
            "Successfully built py-cpuinfo\n",
            "Installing collected packages: sentencepiece, opencv-python-headless, tensorflow-model-optimization, py-cpuinfo, tf-slim, tf-models-official\n",
            "Successfully installed opencv-python-headless-4.4.0.42 py-cpuinfo-7.0.0 sentencepiece-0.1.91 tensorflow-model-optimization-0.4.1 tf-models-official-2.3.0 tf-slim-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuXFFIGNumD4",
        "colab_type": "text"
      },
      "source": [
        "Script Begin:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ3yXM9WXfCZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "44716729-a9bd-48eb-9679-43eabe778ce1"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True, return_X_y=False)\n",
        "newsgroups_test = fetch_20newsgroups(data_home=None, subset='test', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True, return_X_y=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "INFO:sklearn.datasets._twenty_newsgroups:Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
            "INFO:sklearn.datasets._twenty_newsgroups:Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoNRFjVCvCJ3",
        "colab_type": "text"
      },
      "source": [
        "Train Data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbuubPrIUE12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = newsgroups_train.data\n",
        "targets = newsgroups_train.target\n",
        "categories = newsgroups_train.target_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbWP8jbzaDCk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "428adf41-2826-44f9-8371-94b1d439b59d"
      },
      "source": [
        "# Create Feature Inputs -- X\n",
        "data_len = len(data)\n",
        "pC = progressCounter(data_len)\n",
        "bert_inputs_x1 = [0]*data_len\n",
        "bert_inputs_x2 = [0]*data_len\n",
        "bert_inputs_x3 = [0]*data_len\n",
        "\n",
        "for i in range(data_len):\n",
        "  temp = convertToBERTInputData(data[i])\n",
        "  bert_inputs_x1[i] = temp[0]\n",
        "  bert_inputs_x2[i] = temp[1]\n",
        "  bert_inputs_x3[i] = temp[2]\n",
        "  pC.check_pt(i)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Progress: 100%"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW2GmjOJTwSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# Create Feature Outputs -- Y\n",
        "data_len = len(targets)\n",
        "expected_output = [0] * data_len\n",
        "for i in range(data_len):\n",
        "  expected_output[i] = [0]*20\n",
        "\n",
        "for i in range(data_len):\n",
        "  index = targets[i]\n",
        "  expected_output[i][index] = 1\n",
        "  expected_output[i] = np.array(expected_output[i], dtype=np.int32)\n",
        "\n",
        "expected_output_y = expected_output # <----------- y value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLSzMOkOvnip",
        "colab_type": "text"
      },
      "source": [
        "Create TF -- BERT Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et3DykWHMcbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from official import nlp\n",
        "\n",
        "classifier_layer_1 = tf.keras.layers.Dense(512, name='classification_layer_1',activation='relu')(pooled_output)\n",
        "classifier_layer_2 = tf.keras.layers.Dense(256, name='classification_layer_2',activation='relu')(classifier_layer_1)\n",
        "predictions = tf.keras.layers.Dense(20, name='classification_layer_3',activation='sigmoid')(classifier_layer_2)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=bert_inputs, outputs=predictions, name='Bert_Tensorflow')\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics='categorical_accuracy') #add from logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9pStdGS04YF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "9dc93869-3e2c-4318-b536-40e0c75f65b9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Bert_Tensorflow\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer (KerasLayer)         [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "classification_layer_1 (Dense)  (None, 512)          524800      bert_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "classification_layer_2 (Dense)  (None, 256)          131328      classification_layer_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "classification_layer_3 (Dense)  (None, 20)           5140        classification_layer_2[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 335,803,157\n",
            "Trainable params: 335,803,156\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtC72R65l4kD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2d964489-251e-4df2-a50a-9638cabb0514"
      },
      "source": [
        "expected_output_y = np.array(expected_output_y, dtype = np.int32)\n",
        "expected_output_y = np.reshape(expected_output_y,(11314,20))\n",
        "print(expected_output_y.shape)\n",
        "\n",
        "bert_inputs_x1 = np.array(bert_inputs_x1,dtype=np.int32)\n",
        "bert_inputs_x1 = np.reshape(bert_inputs_x1,(11314,512))\n",
        "\n",
        "bert_inputs_x2 = np.array(bert_inputs_x2,dtype=np.int32)\n",
        "bert_inputs_x2 = np.reshape(bert_inputs_x2,(11314,512))\n",
        "\n",
        "bert_inputs_x3 = np.array(bert_inputs_x3,dtype=np.int32)\n",
        "bert_inputs_x3 = np.reshape(bert_inputs_x3,(11314,512))\n",
        "\n",
        "print(bert_inputs_x1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11314, 20)\n",
            "(11314, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJSngHtMeNAq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af8b0118-57d5-4d6e-9034-98ba5639fe69"
      },
      "source": [
        "model.fit({'input_word_ids' : bert_inputs_x1, 'input_mask' : bert_inputs_x2 , 'segment_ids' : bert_inputs_x3 }, {'classification_layer_3' : expected_output_y}, verbose=1, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 3782/11314 [=========>....................] - ETA: 34:27 - loss: 2.9922 - categorical_accuracy: 0.0539"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}